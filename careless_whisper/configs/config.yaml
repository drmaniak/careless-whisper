model_name: "openai/whisper-small.en"
output_dir: "./whisper-diarization-finetuned"
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
eval_strategy: "epoch"
save_strategy: "epoch"
num_train_epochs: 1
learning_rate: 0.0001
fp16: true
logging_steps: 10
wandb_project: "whisper-diarization"
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1
