{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3f5021-6467-493e-af81-b73290252e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c1b164-1e83-4920-a013-2d280173cf85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"edinburghcstr/ami\", \"ihm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3748a7-7941-429e-b951-751d6c8fa1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['meeting_id', 'audio_id', 'text', 'audio', 'begin_time', 'end_time', 'microphone_id', 'speaker_id'],\n",
      "        num_rows: 108502\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['meeting_id', 'audio_id', 'text', 'audio', 'begin_time', 'end_time', 'microphone_id', 'speaker_id'],\n",
      "        num_rows: 13098\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['meeting_id', 'audio_id', 'text', 'audio', 'begin_time', 'end_time', 'microphone_id', 'speaker_id'],\n",
      "        num_rows: 12643\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dda0a61-91d0-4085-a19b-c477af6d76b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meeting_id': 'ES2011a', 'audio_id': 'AMI_ES2011a_H03_FEE044_0092784_0093052', 'text': \"BUT LIKE MOBILE PHONES HAVE SCREENS AND THEY'RE CHEAP\", 'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h03_fee044_0092784_0093052.wav', 'array': array([-1.22070312e-04, -9.15527344e-05, -9.15527344e-05, ...,\n",
      "        1.52587891e-04,  3.05175781e-05,  1.83105469e-04]), 'sampling_rate': 16000}, 'begin_time': 927.8400268554688, 'end_time': 930.52001953125, 'microphone_id': 'H03', 'speaker_id': 'FEE044'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"validation\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118d79d9-808d-42fd-a8ca-737110dde11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "meetings = defaultdict(list)\n",
    "for example in dataset[\"validation\"]:\n",
    "    example[\"duration\"] = abs(example[\"end_time\"] - example[\"begin_time\"])\n",
    "    meetings[example[\"meeting_id\"]].append(example)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be832573-aa23-49ee-be48-c2ad9adad4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the meetings by begin_time\n",
    "for meeting_id in meetings:\n",
    "    meetings[meeting_id].sort(key=lambda x: x[\"begin_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154d02fd-fc4b-40f1-b1ee-e37f31b46808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meeting_id': 'ES2011a',\n",
       "  'audio_id': 'AMI_ES2011a_H00_FEE041_0003427_0003714',\n",
       "  'text': 'HERE WE GO',\n",
       "  'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h00_fee041_0003427_0003714.wav',\n",
       "   'array': array([-2.74658203e-04, -3.05175781e-04, -2.13623047e-04, ...,\n",
       "           9.15527344e-05,  6.10351562e-05,  0.00000000e+00]),\n",
       "   'sampling_rate': 16000},\n",
       "  'begin_time': 34.27000045776367,\n",
       "  'end_time': 37.13999938964844,\n",
       "  'microphone_id': 'H00',\n",
       "  'speaker_id': 'FEE041',\n",
       "  'duration': 2.8699989318847656},\n",
       " {'meeting_id': 'ES2011a',\n",
       "  'audio_id': 'AMI_ES2011a_H00_FEE041_0003714_0003915',\n",
       "  'text': 'WELCOME EVERYBODY',\n",
       "  'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h00_fee041_0003714_0003915.wav',\n",
       "   'array': array([-3.05175781e-05, -3.05175781e-05, -3.05175781e-05, ...,\n",
       "          -9.15527344e-05, -9.15527344e-05, -3.05175781e-05]),\n",
       "   'sampling_rate': 16000},\n",
       "  'begin_time': 37.13999938964844,\n",
       "  'end_time': 39.150001525878906,\n",
       "  'microphone_id': 'H00',\n",
       "  'speaker_id': 'FEE041',\n",
       "  'duration': 2.0100021362304688},\n",
       " {'meeting_id': 'ES2011a',\n",
       "  'audio_id': 'AMI_ES2011a_H00_FEE041_0003915_0004332',\n",
       "  'text': \"UM I'M ABIGAIL CLAFLIN\",\n",
       "  'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h00_fee041_0003915_0004332.wav',\n",
       "   'array': array([-6.10351562e-05, -3.05175781e-05, -3.05175781e-05, ...,\n",
       "           6.10351562e-05,  0.00000000e+00,  0.00000000e+00]),\n",
       "   'sampling_rate': 16000},\n",
       "  'begin_time': 39.150001525878906,\n",
       "  'end_time': 43.31999969482422,\n",
       "  'microphone_id': 'H00',\n",
       "  'speaker_id': 'FEE041',\n",
       "  'duration': 4.1699981689453125},\n",
       " {'meeting_id': 'ES2011a',\n",
       "  'audio_id': 'AMI_ES2011a_H00_FEE041_0004332_0004439',\n",
       "  'text': 'YOU CAN CALL ME ABBIE',\n",
       "  'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h00_fee041_0004332_0004439.wav',\n",
       "   'array': array([ 3.05175781e-05,  0.00000000e+00, -3.05175781e-05, ...,\n",
       "           0.00000000e+00,  6.10351562e-05,  1.52587891e-04]),\n",
       "   'sampling_rate': 16000},\n",
       "  'begin_time': 43.31999969482422,\n",
       "  'end_time': 44.38999938964844,\n",
       "  'microphone_id': 'H00',\n",
       "  'speaker_id': 'FEE041',\n",
       "  'duration': 1.0699996948242188},\n",
       " {'meeting_id': 'ES2011a',\n",
       "  'audio_id': 'AMI_ES2011a_H00_FEE041_0004643_0004763',\n",
       "  'text': \"'S SEE\",\n",
       "  'audio': {'path': '/home/fullldiesel/.cache/huggingface/datasets/downloads/extracted/cc01c8370b8fb423a86dbb2bdd5be3a0aca08a711c26c6e1dc79352850c6207f/ES2011a/dev_ami_es2011a_h00_fee041_0004643_0004763.wav',\n",
       "   'array': array([-6.10351562e-05,  0.00000000e+00, -3.05175781e-05, ...,\n",
       "          -3.05175781e-05, -3.05175781e-05,  0.00000000e+00]),\n",
       "   'sampling_rate': 16000},\n",
       "  'begin_time': 46.43000030517578,\n",
       "  'end_time': 47.630001068115234,\n",
       "  'microphone_id': 'H00',\n",
       "  'speaker_id': 'FEE041',\n",
       "  'duration': 1.2000007629394531}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meetings[\"ES2011a\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b816a9-a9cc-4f53-abdd-2d7610f144a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67200,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meetings[\"ES2011a\"][7][\"audio\"][\"array\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3b417e-a45b-4e07-9402-389b585f8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f22fe4-c116-4c09-b23a-6117c7d8ef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|startoftranscript|>',\n",
       "  '<|en|>',\n",
       "  '<|zh|>',\n",
       "  '<|de|>',\n",
       "  '<|es|>',\n",
       "  '<|ru|>',\n",
       "  '<|ko|>',\n",
       "  '<|fr|>',\n",
       "  '<|ja|>',\n",
       "  '<|pt|>',\n",
       "  '<|tr|>',\n",
       "  '<|pl|>',\n",
       "  '<|ca|>',\n",
       "  '<|nl|>',\n",
       "  '<|ar|>',\n",
       "  '<|sv|>',\n",
       "  '<|it|>',\n",
       "  '<|id|>',\n",
       "  '<|hi|>',\n",
       "  '<|fi|>',\n",
       "  '<|vi|>',\n",
       "  '<|iw|>',\n",
       "  '<|uk|>',\n",
       "  '<|el|>',\n",
       "  '<|ms|>',\n",
       "  '<|cs|>',\n",
       "  '<|ro|>',\n",
       "  '<|da|>',\n",
       "  '<|hu|>',\n",
       "  '<|ta|>',\n",
       "  '<|no|>',\n",
       "  '<|th|>',\n",
       "  '<|ur|>',\n",
       "  '<|hr|>',\n",
       "  '<|bg|>',\n",
       "  '<|lt|>',\n",
       "  '<|la|>',\n",
       "  '<|mi|>',\n",
       "  '<|ml|>',\n",
       "  '<|cy|>',\n",
       "  '<|sk|>',\n",
       "  '<|te|>',\n",
       "  '<|fa|>',\n",
       "  '<|lv|>',\n",
       "  '<|bn|>',\n",
       "  '<|sr|>',\n",
       "  '<|az|>',\n",
       "  '<|sl|>',\n",
       "  '<|kn|>',\n",
       "  '<|et|>',\n",
       "  '<|mk|>',\n",
       "  '<|br|>',\n",
       "  '<|eu|>',\n",
       "  '<|is|>',\n",
       "  '<|hy|>',\n",
       "  '<|ne|>',\n",
       "  '<|mn|>',\n",
       "  '<|bs|>',\n",
       "  '<|kk|>',\n",
       "  '<|sq|>',\n",
       "  '<|sw|>',\n",
       "  '<|gl|>',\n",
       "  '<|mr|>',\n",
       "  '<|pa|>',\n",
       "  '<|si|>',\n",
       "  '<|km|>',\n",
       "  '<|sn|>',\n",
       "  '<|yo|>',\n",
       "  '<|so|>',\n",
       "  '<|af|>',\n",
       "  '<|oc|>',\n",
       "  '<|ka|>',\n",
       "  '<|be|>',\n",
       "  '<|tg|>',\n",
       "  '<|sd|>',\n",
       "  '<|gu|>',\n",
       "  '<|am|>',\n",
       "  '<|yi|>',\n",
       "  '<|lo|>',\n",
       "  '<|uz|>',\n",
       "  '<|fo|>',\n",
       "  '<|ht|>',\n",
       "  '<|ps|>',\n",
       "  '<|tk|>',\n",
       "  '<|nn|>',\n",
       "  '<|mt|>',\n",
       "  '<|sa|>',\n",
       "  '<|lb|>',\n",
       "  '<|my|>',\n",
       "  '<|bo|>',\n",
       "  '<|tl|>',\n",
       "  '<|mg|>',\n",
       "  '<|as|>',\n",
       "  '<|tt|>',\n",
       "  '<|haw|>',\n",
       "  '<|ln|>',\n",
       "  '<|ha|>',\n",
       "  '<|ba|>',\n",
       "  '<|jw|>',\n",
       "  '<|su|>',\n",
       "  '<|translate|>',\n",
       "  '<|transcribe|>',\n",
       "  '<|startoflm|>',\n",
       "  '<|startofprev|>',\n",
       "  '<|nocaptions|>',\n",
       "  '<|notimestamps|>']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00811dc1-d3d8-435a-a6d3-efd8f1ac05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I went to the shop. <|speakerturn|> Oh that's nice! <|speakerturn|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc06e2f-a6ff-41d6-9b12-243175369175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50257, 50362, 40, 1816, 284, 262, 6128, 13, 1279, 91, 47350, 861, 700, 91, 29, 3966, 326, 338, 3621, 0, 1279, 91, 47350, 861, 700, 91, 29, 50256], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(text=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c0cfc4-3add-4126-aab7-0aafe29d0caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'Ġwent',\n",
       " 'Ġto',\n",
       " 'Ġthe',\n",
       " 'Ġshop',\n",
       " '.',\n",
       " 'Ġ<',\n",
       " '|',\n",
       " 'speak',\n",
       " 'ert',\n",
       " 'urn',\n",
       " '|',\n",
       " '>',\n",
       " 'ĠOh',\n",
       " 'Ġthat',\n",
       " \"'s\",\n",
       " 'Ġnice',\n",
       " '!',\n",
       " 'Ġ<',\n",
       " '|',\n",
       " 'speak',\n",
       " 'ert',\n",
       " 'urn',\n",
       " '|',\n",
       " '>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae9ed75-b1d3-4173-bc5d-180f4b2b3981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|speakerturn|>\"]}, replace_additional_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8963893-1645-4de1-992a-ff708e0ff604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>',\n",
       " '<|en|>',\n",
       " '<|zh|>',\n",
       " '<|de|>',\n",
       " '<|es|>',\n",
       " '<|ru|>',\n",
       " '<|ko|>',\n",
       " '<|fr|>',\n",
       " '<|ja|>',\n",
       " '<|pt|>',\n",
       " '<|tr|>',\n",
       " '<|pl|>',\n",
       " '<|ca|>',\n",
       " '<|nl|>',\n",
       " '<|ar|>',\n",
       " '<|sv|>',\n",
       " '<|it|>',\n",
       " '<|id|>',\n",
       " '<|hi|>',\n",
       " '<|fi|>',\n",
       " '<|vi|>',\n",
       " '<|iw|>',\n",
       " '<|uk|>',\n",
       " '<|el|>',\n",
       " '<|ms|>',\n",
       " '<|cs|>',\n",
       " '<|ro|>',\n",
       " '<|da|>',\n",
       " '<|hu|>',\n",
       " '<|ta|>',\n",
       " '<|no|>',\n",
       " '<|th|>',\n",
       " '<|ur|>',\n",
       " '<|hr|>',\n",
       " '<|bg|>',\n",
       " '<|lt|>',\n",
       " '<|la|>',\n",
       " '<|mi|>',\n",
       " '<|ml|>',\n",
       " '<|cy|>',\n",
       " '<|sk|>',\n",
       " '<|te|>',\n",
       " '<|fa|>',\n",
       " '<|lv|>',\n",
       " '<|bn|>',\n",
       " '<|sr|>',\n",
       " '<|az|>',\n",
       " '<|sl|>',\n",
       " '<|kn|>',\n",
       " '<|et|>',\n",
       " '<|mk|>',\n",
       " '<|br|>',\n",
       " '<|eu|>',\n",
       " '<|is|>',\n",
       " '<|hy|>',\n",
       " '<|ne|>',\n",
       " '<|mn|>',\n",
       " '<|bs|>',\n",
       " '<|kk|>',\n",
       " '<|sq|>',\n",
       " '<|sw|>',\n",
       " '<|gl|>',\n",
       " '<|mr|>',\n",
       " '<|pa|>',\n",
       " '<|si|>',\n",
       " '<|km|>',\n",
       " '<|sn|>',\n",
       " '<|yo|>',\n",
       " '<|so|>',\n",
       " '<|af|>',\n",
       " '<|oc|>',\n",
       " '<|ka|>',\n",
       " '<|be|>',\n",
       " '<|tg|>',\n",
       " '<|sd|>',\n",
       " '<|gu|>',\n",
       " '<|am|>',\n",
       " '<|yi|>',\n",
       " '<|lo|>',\n",
       " '<|uz|>',\n",
       " '<|fo|>',\n",
       " '<|ht|>',\n",
       " '<|ps|>',\n",
       " '<|tk|>',\n",
       " '<|nn|>',\n",
       " '<|mt|>',\n",
       " '<|sa|>',\n",
       " '<|lb|>',\n",
       " '<|my|>',\n",
       " '<|bo|>',\n",
       " '<|tl|>',\n",
       " '<|mg|>',\n",
       " '<|as|>',\n",
       " '<|tt|>',\n",
       " '<|haw|>',\n",
       " '<|ln|>',\n",
       " '<|ha|>',\n",
       " '<|ba|>',\n",
       " '<|jw|>',\n",
       " '<|su|>',\n",
       " '<|translate|>',\n",
       " '<|transcribe|>',\n",
       " '<|startoflm|>',\n",
       " '<|startofprev|>',\n",
       " '<|nocaptions|>',\n",
       " '<|notimestamps|>',\n",
       " '<|speakerturn|>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.special_tokens_map[\"additional_special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3496035c-65f0-431f-a8a3-ac56be40b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_transcripts(utterances):\n",
    "    merged_text = \"\"\n",
    "    prev_speaker = None\n",
    "    for utt in utterances:\n",
    "        current_speaker = utt[\"speaker_id\"]\n",
    "        # Insert a speaker change token in the speaker changes\n",
    "        if prev_speaker is not None and current_speaker != prev_speaker:\n",
    "            merged_text += \" <|speakerturn|> \"\n",
    "        merged_text += utt[\"text\"].strip() + \" \"\n",
    "        prev_speaker = current_speaker\n",
    "    return merged_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271320d7-602f-4f09-9d3b-d9df93f96e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_audio(utterances, sampling_rate=16000, gap_duration=0.1):\n",
    "    gap_samples = int(gap_duration * sampling_rate)\n",
    "    silence = np.zeros(gap_samples, dtype=np.float32)\n",
    "    merged_audio_segments = []\n",
    "\n",
    "    for utt in utterances:\n",
    "        audio_array = utt[\"audio\"][\"array\"]\n",
    "        merged_audio_segments.append(audio_array)\n",
    "        merged_audio_segments.append(silence)\n",
    "\n",
    "    if merged_audio_segments:\n",
    "        merged_audio_segments = merged_audio_segments[:-1]\n",
    "\n",
    "    return np.concatenate(merged_audio_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d7ea4d9-6d97-4dc7-9bb8-3e91d4828aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_meeting(utterances, target_duration=25.0, max_duration=27.0):\n",
    "    \"\"\"\n",
    "    Group utterances into segments of roughly target_duration to max_duration seconds.\n",
    "    Each segment will have both merged text and merged audio.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    current_duration = 0.0\n",
    "\n",
    "    for utt in utterances:\n",
    "        utt_duration = utt[\"duration\"]\n",
    "        if current_duration + utt_duration <= max_duration:\n",
    "            current_segment.append(utt)\n",
    "            current_duration += utt_duration\n",
    "            # If we have reached at least the target duration, finalize the segment.\n",
    "            if current_duration >= target_duration:\n",
    "                segments.append(current_segment)\n",
    "                current_segment = []\n",
    "                current_duration = 0.0\n",
    "        else:\n",
    "            # If current segment is below target but adding the utterance exceeds max_duration,\n",
    "            # you can choose to add it anyway (slight overage) or finalize the segment.\n",
    "            if current_duration >= target_duration:\n",
    "                segments.append(current_segment)\n",
    "                current_segment = [utt]\n",
    "                current_duration = utt_duration\n",
    "            else:\n",
    "                current_segment.append(utt)\n",
    "                current_duration += utt_duration\n",
    "                segments.append(current_segment)\n",
    "                current_segment = []\n",
    "                current_duration = 0.0\n",
    "\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c232f66b-e786-44f7-a016-d4959577a5b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1: Duration = 26.26s\n",
      "Merged Text: HERE WE GO WELCOME EVERYBODY UM I'M ABIGAIL CLAFLIN YOU CAN CALL ME ABBIE 'S SEE POWERPOINT THAT'S NOT IT THERE WE GO SO THIS IS OUR KICK OFF MEETING UM AND I GUESS WE SHOULD ALL GET ACQUAINTED LET'S SHALL WE ALL INTRODUCE OURSELVES\n",
      "Segment 2: Duration = 26.75s\n",
      "Merged Text: HI I'M CHIARA I'M THE UM MARKETING EXPERT UM WOULD YOU LIKE ME TO TALK ABOUT MY AIMS AT THE MOMENT OR WOULD YOU LIKE ME TO JUST SAY MY NAME AND THEN WE CAN TALK ABOUT BUSINESS LATER  <|speakerturn|> I THINK WE'LL GET AROUND TO THAT YEAH  <|speakerturn|> WE'LL GET ROUND TO THAT LATER  <|speakerturn|> SO THIS IS JUST INTRODUCTIONS YEAH  <|speakerturn|> MY NAME IS CHIARA AND I'M THE MARKETING EXPERT  <|speakerturn|> OKAY I FORGOT TO S SAY I'M THE PROJECT MANAGER BUT I FIGURED YOU ALL KNEW THAT ALREADY UM SO\n",
      "Segment 3: Duration = 31.37s\n",
      "Merged Text: I'M STEPHANIE AND I AM THE USER INTERFACE DESIGNER  <|speakerturn|> I'M KRISTA AND I'M THE INDUSTRIAL DESIGNER  <|speakerturn|> OKAY UM SO F HERE'S OUR AGENDA FOR TODAY UM WE'RE GONNA DO SOME TOOL TRAINING PROJECT PLAN AND DISCUSS THEN CLOSE UM SO SO OUR AIM IS TO PRODUCE A REMOTE CONTROL THAT IS ORIGINAL TRENDY AND USER FRIENDLY\n",
      "Segment 4: Duration = 26.60s\n",
      "Merged Text: AND TO DO THIS WE HAVE TO UM THERE'S CERTAIN THINGS WE HAVE TO CONSIDER ABOUT FUNCTIONAL ASPECTS AND CONCEPTUAL DESIGN OF THE THING SO WE'LL GET TO THAT OH THERE IT IS RIGHT FUNCTIONAL DESIGN CONCEPTUAL DESIGN AND DETAILED DESIGN SO THROUGHOUT OUR NEXT COUPLE OF MEETINGS WE'LL WE'LL BE COVERING THESE THINGS\n",
      "Segment 5: Duration = 26.34s\n",
      "Merged Text: UM SO WE'RE GONNA TRY OUT OUR WHITE BOARD IF WE'LL ALL DRAW OUR FAVOURITE ANIMAL TO SUM UP THE CHARACTERISTICS OF THAT ANIMAL  <|speakerturn|> SO YOU WANT US TO DRAW IT AND THEN TALK ABOUT IT  <|speakerturn|> OKAY  <|speakerturn|> OR JUST DRAW IT  <|speakerturn|> I THINK BOTH  <|speakerturn|> OKAY  <|speakerturn|> YEAH  <|speakerturn|> WHY DON'T WE DO BOTH  <|speakerturn|> BOTH YEAH  <|speakerturn|> WHO STARTS  <|speakerturn|> RIGHT  <|speakerturn|> WE OUGHT TO DECIDE WHO STARTS AND ALL THAT\n",
      "Segment 6: Duration = 28.49s\n",
      "Merged Text: NO  <|speakerturn|> ANY VOLUNTEERS  <|speakerturn|> UH-HUH  <|speakerturn|> DOES ANYONE KNOW WHAT THEY WANNA DRAW  <|speakerturn|> MM I GOTTA THINK ABOUT IT FOR A SECOND LIKE UH DOES IT HAVE TO BE FUNCTIONAL TRENDY AND USER FRIENDLY  <|speakerturn|> I DON'T THINK SO  <|speakerturn|> UM OKAY I'LL DRAW I'LL DRAW ONE MAKE SURE MY THINGS HERE UH OH RIGHT OKAY MY FAVOURITE ANIMAL IS SEE OOPS  <|speakerturn|> A DOLPHIN\n",
      "Segment 7: Duration = 31.87s\n",
      "Merged Text: YEAH IT'S  <|speakerturn|> 'S LIKE PLAYING PICTIONARY  <|speakerturn|> YEAH I GUESS IT HAS A FIN ON TOP TOO YEAH IT'S MY DOLPHIN  <|speakerturn|> SO WHAT CHARACTERISTICS DO YOU LIKE ABOUT YOUR ANIMAL  <|speakerturn|> I LIKE ITS TAIL UM NO I THINK DOLPHINS ARE REALLY UH I DUNNO THEY'RE SMART AND THEY THEY'RE CUTE AND THEY LIKE SWIMMING AND THAT'S COOL LIKE THEY'RE GRACEFUL YEAH AND THEY'RE SO\n",
      "Segment 8: Duration = 42.42s\n",
      "Merged Text: THEY'RE GRACEFUL SLEEK  <|speakerturn|> YEAH THEY'RE SLEEK AND THEY LOOK INTELLIGENT AND I DON'T KNOW THEY'RE I GUESS IT'S THE WHOLE LIKE BINOCULAR VISION THING  <|speakerturn|> I DON'T KNOW HOW INTELLIGENT THAT ONE LOOKS  <|speakerturn|> YEAH HE HE DOESN'T LOOK THAT SMART HE'S A I DUNNO UM THEY'RE I THINK IT'S COOL THE THE UM THE INTERACTION THAT OR THE TH THINGS THAT THE REASONS PEOPLE SEEM TO LIKE YOU KNOW YOU GET EX YOU KNOW PEOPLE ARE SITTING ON THE BEACH AND P THEY'RE LIKE OH LOOK THERE'S DOLPHINS AND IT'S KINDA LIKE BUT THEY'RE YOU KNOW THEY JUMP AROUND IN THE WATER AND THEY'RE HAPPY AND THEY'RE MAMMALS\n",
      "Segment 9: Duration = 26.93s\n",
      "Merged Text: BUT THEY SWIM  <|speakerturn|> YES DOES ANYBODY ELSE WANNA DRAW THEIR ANIMAL  <|speakerturn|> SUPPOSE I CAN DRAW AN ANIMAL YEAH  <|speakerturn|> UH OH THERE GOES THE TEN  <|speakerturn|> YEAH  <|speakerturn|> IT'S A CAT  <|speakerturn|> I DON'T KNOW THEY SLEEP ALL DAY THEY'RE EASY TO DRAW  <|speakerturn|> DO YOU WANNA ANYTHING  <|speakerturn|> UH YEAH  <|speakerturn|> I DUNNO IF THE THE AH\n",
      "Segment 10: Duration = 27.70s\n",
      "Merged Text: WELL I HAD THE CAT AS WELL BUT UH I'VE GOT A SPARE ONE  <|speakerturn|> I THINK THE PEN IS RUNNING OUT OF  <|speakerturn|> SO I'LL USE THE SPARE ONE UM BUT IT'S HARDER TO DRAW UM  <|speakerturn|> AND THE PEN'S DYING  <|speakerturn|> UM  <|speakerturn|> A HORSE  <|speakerturn|> UH  <|speakerturn|> HORSE  <|speakerturn|> UM I DON'T REALLY KNOW HOW THE LEGS GO BUT ANYWAY I WILL DO THAT  <|speakerturn|> THAT'S VERY GOOD  <|speakerturn|> UM AND THE MAIN REASON IS THEY'RE PRETTY\n",
      "Segment 11: Duration = 25.24s\n",
      "Merged Text: I THINK THEY'RE VERY PRETTY AND THEY GO WELL WITH THE ENVIRONMENT AND I LIKE THE WAY THEY RUN AND I USED TO DO HORSE RIDING AND THEY'RE JUST VERY SORT OF STURDY AND NICE ANIMALS AND I LIKE THE WAY UM THEY FEEL SORT OF UNDER UNDER THE HAND I THINK THAT'S PRETTY MUCH IT UM  <|speakerturn|> YEAH THIS CORD'S UH RIGHT\n",
      "Segment 12: Duration = 27.91s\n",
      "Merged Text: ACTUALLY I HAVEN'T THOUGHT OF ANYTHING YET UH IT'S A PIG SO I'M THINKING WE SHOULD DESIGN A REMOTE CONTROL THAT'S WATER RESISTANT STRONG AND FURRY WHAT DO YOU THINK YEAH  <|speakerturn|> AND FURRY  <|speakerturn|> THIS IS YEAH WELL LIKE A CAT YOU KNOW SOFT YEAH  <|speakerturn|> TEXTILE TACTILE TACTILE REMOTE CONTROL\n",
      "Segment 13: Duration = 29.91s\n",
      "Merged Text: ALTHOUGH UH I'LL JUST PUT THERE RIGHT  <|speakerturn|> YOU'RE DRAGGING A YOU HAVE A TAIL  <|speakerturn|> OH MY GOSH THIS IS DISASTROUS SORRY ABOUT THAT OKAY SO MOVING ON OUR SELLING PRICE GOAL IS TWENTY FIVE EURO AND PROFIT AIM IS FIFTY MILLION EURO\n",
      "Segment 14: Duration = 26.11s\n",
      "Merged Text: SO I'M GUESSING THAT WE'RE NOT ACTUALLY IN SCOTLAND WE'RE IN SOME EUROPEAN COUNTRY UM AND WE WILL HOPE TO SELL THIS INTERNATIONALLY  <|speakerturn|> SORRY CAN YOU JUST SAY THAT WHAT'S THE WHAT ARE OUR PRICE GOALS AGAIN  <|speakerturn|> UM SELLING PRICE IS TWENTY FIVE EURO  <|speakerturn|> OKAY  <|speakerturn|> PROFIT AIM FIFTY MILLION EURO  <|speakerturn|> HOW MANY SHOULD WE SELL THEN\n",
      "Segment 15: Duration = 31.00s\n",
      "Merged Text: UM A LOT TWO TWO TWO MILLION TWO MI NO MORE F FOUR MILLION  <|speakerturn|> ANYONE A MATHEMATICIAN  <|speakerturn|> TWO MILLION  <|speakerturn|> FOUR MILLION AND IT WELL IT'S THE PROFIT SO IF A PROFIT FOR EACH IS TWELVE FIFTY THAT'LL DO FOUR MILLION  <|speakerturn|> OH YEAH  <|speakerturn|> IT IS A LOT UH  <|speakerturn|> SO F THAT'S A FIFTY PERCENT UM UH UM I DON'T KNOW WHAT THESE MEAN BECAUSE I DIDN'T ACTUALLY MAKE THE SLIDE SHOW\n",
      "Segment 16: Duration = 27.69s\n",
      "Merged Text: EXPERIENCE WITH REMOTE CONTROL SO I GUESS WE HAVE TO REFLECT ON OUR EXPERIENCES WITH REMOTE CONTROLS TO DECIDE WHAT UM WE WOULD LIKE TO SEE IN A CONVENIENT PRACTICAL NICE REMOTE CONTROL UM SO DO WE HAVE ANY INITIAL IDEAS FOR UH HOW THIS REMOTE CONTROL SHOULD BE DESIGNED OR FORMATTED OR THE THE BUTTONS IT SHOULD HAVE\n",
      "Segment 17: Duration = 25.24s\n",
      "Merged Text: UM I THINK ONE THING IS THAT IT SHOULD BE EASY TO FIND BEC YEAH BEC  <|speakerturn|> I WAS THINKING THAT TOO  <|speakerturn|> YEAH  <|speakerturn|> I THINK WE SHOULD DESIGN SOMETHING THAT HAS LIKE A SO YOU CAN LIKE SOMEHOW LIKE YOU I MEAN YOU ALWAYS KNOW WHERE YOUR T. V. IS SO JUST HAVE A CALL BUTTON I'VE ALWAYS WANTED THAT  <|speakerturn|> YEAH YEAH YEAH YEAH YEAH  <|speakerturn|> SO LIKE YOU CAN PUSH A BUTTON ON YOUR T. V. YEAH YEAH YEAH YEAH  <|speakerturn|> YEAH I MEAN YOU HAVE IT FOR THE PORTABLE PHONE SO WHY NOT YEAH\n",
      "Segment 18: Duration = 32.11s\n",
      "Merged Text: SO YOU SHOULD HAVE A CALL BUTTON ON YOUR TELEVISION TO BE ABLE TO FIND YOUR REMOTE CONTROL  <|speakerturn|> YEAH AND EVEN I THINK A LITTLE LIGHT UM OR EVEN A MAYBE A VIB A VIBRATING THING I DUNNO BUT SOMETH BECAUSE IT'S USUALLY UNDER THE SOFA  <|speakerturn|> YEAH  <|speakerturn|> IN WHICH CASE YOU'RE GOING TO BE L BUT IF IT HAS A SORT OF SIGNAL WHICH ISN'T ANY SOUND I DON'T KNOW IF IT'S EXPENSIVE MAYBE TO  <|speakerturn|> YEAH YEAH I DON'T YEAH I MEAN IT BUT LIKE I MEAN JUST I MEAN LIKE YOUR PHONE EVEN JUST HAS SO LIKE IT CAN VIBRATE\n",
      "Segment 19: Duration = 25.88s\n",
      "Merged Text: MAYBE CALL IS ENOUGH BUT YEAH YEAH YEAH YEAH  <|speakerturn|> IT CAN LIGHT UP AND MAKE NOISE AND I DUNNO  <|speakerturn|> YEAH  <|speakerturn|> WHAT IF IT HAD SOMETHING LIKE UM JUST LIKE A MAGNET ON THE BACK OF IT AND YOU COULD I MEAN J JUST TO HAVE SOME PLACE TO PUT IT BESIDES LIKE A BASE  <|speakerturn|> YEAH  <|speakerturn|> YOU KNOW LIKE A PORTABLE PHONE HAS A BASE LIKE JUST TO HAVE A HOME FOR IT\n",
      "Segment 20: Duration = 28.03s\n",
      "Merged Text: YEAH OR IF IT HAD A YEAH  <|speakerturn|> YEAH YEAH YEAH YEAH  <|speakerturn|> YEAH I MEAN  <|speakerturn|> 'CAUSE PEOPLE JUST STICK IT ON TOP OF THEIR T. V. BUT THE POINT OF HAVING A REMOTE IS NOT TO HAVE TO WALK OVER TO THE T. V. SO  <|speakerturn|> WELL THAT'S WHY IT'S ALWAYS IN THE COUCH  <|speakerturn|> YEAH  <|speakerturn|> YEAH  <|speakerturn|> YEAH IN IN THE COUCH I DUNNO IT SEEMS LIKE THOUGH THAT THAT WOULD BE HARD 'CAUSE YOU NOT YOU'RE NOT GONNA BE LAZY ANYWAY AND  <|speakerturn|> YEAH MAYBE WE SHOULD DESIGN COUCHES THAT HAVE THE REMOTE CONTROL IN THE SIDE ARM\n",
      "Segment 21: Duration = 25.81s\n",
      "Merged Text: YEAH SO WE THE PROJECT IS NOW COUCHES AND REMOTE CONTROLS  <|speakerturn|> BUT EVEN JUST A THING TO ATTACH IT TO THE W YOU KNOW IF YOU HAD A THING A PRETTY OBJECT ATTACHED TO THE WALL BUT THAT WOULD REALLY MAKE IT MORE EXPENSIVE  <|speakerturn|> YEAH  <|speakerturn|> BUT IT'S ONLY A PLASTIC THING R REALLY THE THING ON THE WALL  <|speakerturn|> YEAH  <|speakerturn|> YEAH  <|speakerturn|> SOMETHING LIKE THAT AND THE OTHER THING IS  <|speakerturn|> DO YOU THINK IT NEEDS TO BE BIGGER TO NOT LOSE OR DOES THAT NOT FACTOR IN\n",
      "Segment 22: Duration = 26.47s\n",
      "Merged Text: BIGGER  <|speakerturn|> NOT WELL IT NEEDS TO BE SORT OF  <|speakerturn|> LIKE HAND HAND HELD SIZE YEAH  <|speakerturn|> HAND SIZED  <|speakerturn|> YEAH I DON'T THINK YOU NEED A  <|speakerturn|> NOT NOT HUGE BUT  <|speakerturn|> BUT DEFINITELY NOT WELL I DON'T KNOW  <|speakerturn|> IT CAN'T BE THAT HARD TO PUT SOME KIND OF A NOISE ON IT  <|speakerturn|> NO IT CAN'T BE UH UH  <|speakerturn|> NO IT REALLY WOULDN'T BE  <|speakerturn|> OR LIKE OR LIKE A LIGHT THING  <|speakerturn|> HUH  <|speakerturn|> YOU KNOW  <|speakerturn|> LIKE SPACESHIP  <|speakerturn|> I DUNNO\n",
      "Segment 23: Duration = 26.25s\n",
      "Merged Text: YEAH  <|speakerturn|> RIGHT UM  <|speakerturn|> OR MAKE IT MOBILE SO IT RUNS AROUND AND COMES COME FIND YOU YEAH THAT WOULD BE REALLY I'M SURE WE COULD DO THAT FOR TWENTY FIVE EUROS A POP  <|speakerturn|> LITTLE HOMING DEVICE  <|speakerturn|> YEAH UH  <|speakerturn|> UM OKAY SO WHAT DO WE THINK THIS REMOTE CONTROL SHOULD FIVE MINUTES  <|speakerturn|> OH DEAR  <|speakerturn|> TILL THE MEETING OH RIGHT\n",
      "Segment 24: Duration = 28.64s\n",
      "Merged Text: THIS IS WHAT WE HAVE LEFT  <|speakerturn|> I ALSO THINK THOUGH THAT IT SHOULDN'T HAVE TOO MANY BUTTONS  <|speakerturn|> UM OH WE JUST  <|speakerturn|> 'CAUSE I HATE THAT WHEN THEY HAVE TOO MANY BUTTONS AND I MEAN I KNOW IT HAS TO HAVE ENOUGH FUNCTIONS BUT LIKE  <|speakerturn|> YEAH I AGREE B. BUTTON AND THE F. BUTTON THEY DON'T DO ANYTHING  <|speakerturn|> YEAH YEAH YEAH  <|speakerturn|> YEAH  <|speakerturn|> I DON'T KNOW YOU JUST HAVE LIKE EIGHT THOUSAND BUTTONS AND YOU'RE LIKE NO YOU NEVER USE HALF OF THEM  <|speakerturn|> YOU WHAT IF UM MAY BE A LITTLE FANCY BUT WHAT IF IT HAD LIKE A LITTLE SCREEN\n",
      "Segment 25: Duration = 28.97s\n",
      "Merged Text: SO  <|speakerturn|> SO IT HAS LESS BUTTONS BUT IT STILL HAS ALL THE FUNCTIONS  <|speakerturn|> THAT WOULD BE COOL  <|speakerturn|> LIKE THE WAY A MOBILE PHONE DOES  <|speakerturn|> YEAH  <|speakerturn|> YEAH I MEAN IT JUST SEEMS LIKE YEAH  <|speakerturn|> SO YOU COULD LIKE UM LIKE IF YOU HAVE I DUNNO IF YOU HAVE SATELLITE IF YOU HAVE A HUNDRED CHANNELS YOU CAN THE WAY YOU DO IT ON YOUR RADIO IS THAT YOU UH WHAT DO YOU CALL IT S Y YEAH BUT YOU CAN PROGRAMME  <|speakerturn|> SELECT UH  <|speakerturn|> SO YOU CAN PROGRAMME LIKE YOUR FAVOURITE CHANNELS SO LIKE IF YOU HAD A S\n",
      "Segment 26: Duration = 26.79s\n",
      "Merged Text: BUT WOULD YOU HAVE THE SCREEN ON THE THING OR WOULD YOU HAVE IT ON THE TELLY TRANSMITTING THE SCREEN  <|speakerturn|> THAT'S SOMETHING WE COULD DECIDE  <|speakerturn|> I GUESS THEY WOULD GO TOGETHER SOMEHOW  <|speakerturn|> BECAUSE  <|speakerturn|> MM  <|speakerturn|> I DUNNO  <|speakerturn|> BECAUSE I DON'T KNOW IF IT'S I THINK IT'S E EXPENSIVE IF YOU HAVE IF YOU USE THE TELLY SCREEN 'CAUSE THE TELLY'S ALREADY A SCREEN  <|speakerturn|> YEAH  <|speakerturn|> THEN YOU CAN PRO SORT OF HAVE A PROGRAMMING FUNCTION REALLY EASY SORT OF ARROW UP AND DOWN ON THE REMOTE AND THEN USE THE TELLY AS A SCREEN\n",
      "Segment 27: Duration = 32.39s\n",
      "Merged Text: YEAH RIGHT  <|speakerturn|> BUT UM  <|speakerturn|> I'M THINKING KIND OF  <|speakerturn|> BUT YEAH FOR SURE SOMETHING LIKE NOT IT'S NOT ON THE BUTTON BUT IT'S TELLING YOU WHAT TO DO IS THAT WHAT YOU MEAN  <|speakerturn|> YEAH YEAH  <|speakerturn|> RIGHT MM  <|speakerturn|> OR LIKE YOU H YOU SEE THOSE YOU KNOW PEOPLE I'M THINKING OF LIKE CELEBRITY CRIBS KIND OF THINGS WHEN LIKE THEY HAVE ALL THOSE THESE THINGS THAT AT THEIR HOUSE YOU KNOW THEIR THEIR ENTIRE HOUSE IS SO ELECTRONIC AND THEY HAVE LIKE THIS ONE MASTER CONTROL THAT AND IT'S LIKE A HAND HELD LIKE TURNS ON EVERYTHING SORT OF CONTROL AND IT HAS LIKE A SCREEN AND LIKE SO I THINK IT SHOULD BE POSSIBLE TO HAVE SOME KIND OF A SCREEN\n",
      "Segment 28: Duration = 25.37s\n",
      "Merged Text: I DON'T KNOW IF IT MUST BE IT WOULD PROBABLY MUST BE EX TOO EXPENSIVE THOUGH T LIKE I DUNNO  <|speakerturn|> YEAH BUT LIKE MOBILE PHONES HAVE SCREENS AND THEY'RE CHEAP  <|speakerturn|> MM  <|speakerturn|> YEAH YEAH YEAH THAT'S TRUE  <|speakerturn|> YEAH I MEAN WE HAVE TO REMEMBER OUR BUDGET IS TWELVE POINT TWELVE FIFTY FOR TO ACTUALLY MAKE THE DEVICE  <|speakerturn|> YEAH  <|speakerturn|> MM  <|speakerturn|> UM BUT IT'S SOMETHING TO THINK ABOUT YEAH  <|speakerturn|> WELL I GUESS WE HAVE TO GET TO THAT LATER YEAH  <|speakerturn|> I MEAN WE'LL HAVE TO SEE HOW MUCH THAT WOULD BE\n",
      "Segment 29: Duration = 31.18s\n",
      "Merged Text: OR SOME IT I WE CAN FIND OUT PROBABLY ON THE INTERNET HOW MUCH IT'S  <|speakerturn|> YEAH  <|speakerturn|> UM YEAH AND THE OTHER THING YOU SAID THAT THING ABOUT ROBUST AND WATER UM WHAT WAS THE WORD  <|speakerturn|> FURRY  <|speakerturn|> WATER RESISTANT  <|speakerturn|> OH I WAS JUST  <|speakerturn|> NO BUT IT'S I THOUGHT AH SPOT ON GOOD FEEL TACT TACTILE GOOD TACTILE FEEL MAYBE SOMETHING DIDN DOESN'T MAKE YOUR HANDS SWEAT LOT\n",
      "Segment 30: Duration = 28.07s\n",
      "Merged Text: YEAH  <|speakerturn|> YEAH MM MM  <|speakerturn|> THAT'S QUITE ANNOYING  <|speakerturn|> MAYBE UM JUST LIKE A SIMPLE THING TO HAVE A CLIP ON IT LIKE SO YOU CAN CLIP IT TO YOUR LIKE THAT'S ANOTHER  <|speakerturn|> YEAH  <|speakerturn|> YEAH CLIP OOH UM  <|speakerturn|> UM WE SHOULD PROBABLY START WRAPPING UP UM WE'VE GOT SOME INITIAL IDEAS THAT WE CAN ALL LOOK INTO UM AND COME UP WITH SOME NEW ONES FOR THE NEXT MEETING WHICH WILL BE IN ANOTHER THIRTY MINUTES\n",
      "Segment 31: Duration = 26.63s\n",
      "Merged Text: UM SO YEAH THE INDUSTRIAL DESIGNER WHAT DOES THAT STAND FOR I. D. YEAH UM IS GOING TO BE LOOKING MORE INTO THE WORKING DESIGN  <|speakerturn|> YEAH I THINK SO  <|speakerturn|> SO I GUESS YOU'D BE LOOKING AT LOTS OF THE THINGS WE DISCUSSED ABOUT SCREEN AND UM THAT SORT OF THING THE SOMETHING WHAT IS THE U. I.  <|speakerturn|> USER  <|speakerturn|> THAT'S ME  <|speakerturn|> YEAH WHAT DOES IT STAND FOR AGAIN  <|speakerturn|> UH USER INTERFACE DESIGN\n",
      "Segment 32: Duration = 27.03s\n",
      "Merged Text: USER INTERFACE DESIGNER SO THAT'S GONNA BE MORE TECHNICAL I GUESS THAT MAYBE THE WORKING DESIGN HAS ALSO TO DO WITH LIKE THE PHYSICAL FEAT LIKE JUST THE WAY IT LOOKS AND THE WAY IT W  <|speakerturn|> SO TECHNICAL FUNCTION  <|speakerturn|> THE WORKING DESIGN IS THE STRUCTURE  <|speakerturn|> YEAH YEAH  <|speakerturn|> WHAT IS TECHNICAL FUNCTIONS EXACTL I I DON'T REALLY KNOW WHAT  <|speakerturn|> UM I GUESS YOU'D HAVE TO FIND OUT  <|speakerturn|> IT SAYS ON THAT EMAIL BUT IT\n",
      "Segment 33: Duration = 27.70s\n",
      "Merged Text: UM IT WAS IN THE EMAIL  <|speakerturn|> IT DOES BUT IT I JUST DON'T REALLY  <|speakerturn|> I WROTE DOWN WHAT MINE WERE  <|speakerturn|> IT SAID UM  <|speakerturn|> YEAH  <|speakerturn|> IT SAID  <|speakerturn|> WHAT EFFECT SHOULD THE THING HA SHOULD IT HAVE OKAY ALRIGHT  <|speakerturn|> YEAH LIKE  <|speakerturn|> AND WORKING DESIGN OKAY  <|speakerturn|> BE A MEDIUM BETWEEN YOU AND THE TELLY I THINK THAT'S UH  <|speakerturn|> YEAH YEAH  <|speakerturn|> MM  <|speakerturn|> ALRIGHT AND HOW IT WORKS OKAY RIGHT I'M I'M ON TASK  <|speakerturn|> AND THE M. E. WHAT DOES THAT STAND FOR\n",
      "Segment 34: Duration = 18.93s\n",
      "Merged Text: M  <|speakerturn|> MARKETING  <|speakerturn|> MARKETING RIGHT  <|speakerturn|> MARKETING OH IT'S WRITTEN HERE BUT UM  <|speakerturn|> UM SO WE'LL BE WORKING ON THE USER REQUIREMENTS UM YEAH  <|speakerturn|> OKAY  <|speakerturn|> SO I GUESS THAT WRAPS IT UP I'LL SEE YOU ALL IN THIRTY MINUTES I JUST DID\n"
     ]
    }
   ],
   "source": [
    "# Example: process one meeting\n",
    "meeting_id = \"ES2011a\"  # For example\n",
    "meeting_utterances = meetings[meeting_id]\n",
    "segments = segment_meeting(meeting_utterances, target_duration=25.0, max_duration=27.0)\n",
    "\n",
    "# Check one segment\n",
    "for i, segment in enumerate(segments):\n",
    "    total_duration = sum(utt[\"duration\"] for utt in segment)\n",
    "    print(f\"Segment {i+1}: Duration = {total_duration:.2f}s\")\n",
    "    print(\"Merged Text:\", merge_transcripts(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a6a142-504c-4d4f-8cf3-adf938969175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98005a6e-4c94-4d41-a545-92c728698153",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = []\n",
    "for meeting_id, utterances in meetings.items():\n",
    "    segments = segment_meeting(utterances, target_duration=24.0, max_duration=26.0)\n",
    "    for i, segment in enumerate(segments):\n",
    "        merged_text = merge_transcripts(segment)\n",
    "        merged_audio = merge_audio(segment, sampling_rate=16000)\n",
    "        segment_duration = sum(utt[\"duration\"] for utt in segment)\n",
    "        training_samples.append({\n",
    "            \"meeting_id\": meeting_id,\n",
    "            \"position\": i,\n",
    "            \"audio\": {\"array\": merged_audio, \"sampling_rate\": 16000},\n",
    "            \"text\": merged_text,\n",
    "            \"duration\": segment_duration,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ab4d021-7102-40a2-b270-816b193b1f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1230"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1bbf068-96ea-4d24-b241-f6df7c47d355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'meeting_id': 'ES2011a',\n",
       "  'position': 0,\n",
       "  'audio': {'array': array([-2.74658203e-04, -3.05175781e-04, -2.13623047e-04, ...,\n",
       "           9.15527344e-05,  9.15527344e-05,  9.15527344e-05]),\n",
       "   'sampling_rate': 16000},\n",
       "  'text': \"HERE WE GO WELCOME EVERYBODY UM I'M ABIGAIL CLAFLIN YOU CAN CALL ME ABBIE 'S SEE POWERPOINT THAT'S NOT IT THERE WE GO SO THIS IS OUR KICK OFF MEETING UM AND I GUESS WE SHOULD ALL GET ACQUAINTED\",\n",
       "  'duration': 24.149993896484375},\n",
       " {'meeting_id': 'ES2011a',\n",
       "  'position': 1,\n",
       "  'audio': {'array': array([ 9.15527344e-05,  6.10351562e-05,  3.05175781e-05, ...,\n",
       "          -4.57763672e-04, -5.18798828e-04, -2.74658203e-04]),\n",
       "   'sampling_rate': 16000},\n",
       "  'text': \"LET'S SHALL WE ALL INTRODUCE OURSELVES  <|speakerturn|> HI I'M CHIARA I'M THE UM MARKETING EXPERT UM WOULD YOU LIKE ME TO TALK ABOUT MY AIMS AT THE MOMENT OR WOULD YOU LIKE ME TO JUST SAY MY NAME AND THEN WE CAN TALK ABOUT BUSINESS LATER  <|speakerturn|> I THINK WE'LL GET AROUND TO THAT YEAH  <|speakerturn|> WE'LL GET ROUND TO THAT LATER  <|speakerturn|> SO THIS IS JUST INTRODUCTIONS YEAH  <|speakerturn|> MY NAME IS CHIARA AND I'M THE MARKETING EXPERT  <|speakerturn|> OKAY I FORGOT TO S SAY I'M THE PROJECT MANAGER BUT I FIGURED YOU ALL KNEW THAT ALREADY UM SO\",\n",
       "  'duration': 28.85999298095703}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6738c532-ed3c-44f3-8a4c-b8d6486b5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Value, Features, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41bfd3a0-ea10-490f-9b9f-4d0d42a000da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(samples):\n",
    "    for sample in samples:\n",
    "        yield sample\n",
    "features = Features({\n",
    "    \"meeting_id\": Value(\"string\"),\n",
    "    \"position\": Value(\"int32\"),\n",
    "    \"audio\": Audio(sampling_rate=16000),\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"duration\": Value(\"float32\")\n",
    "})\n",
    "ds = Dataset.from_generator(lambda: sample_generator(training_samples), features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "706d662c-6cd4-48a6-9308-e96aa5388b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-tiny.en\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e37b0801-9312-492e-a9ef-5b6788f82da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>',\n",
       " '<|en|>',\n",
       " '<|zh|>',\n",
       " '<|de|>',\n",
       " '<|es|>',\n",
       " '<|ru|>',\n",
       " '<|ko|>',\n",
       " '<|fr|>',\n",
       " '<|ja|>',\n",
       " '<|pt|>',\n",
       " '<|tr|>',\n",
       " '<|pl|>',\n",
       " '<|ca|>',\n",
       " '<|nl|>',\n",
       " '<|ar|>',\n",
       " '<|sv|>',\n",
       " '<|it|>',\n",
       " '<|id|>',\n",
       " '<|hi|>',\n",
       " '<|fi|>',\n",
       " '<|vi|>',\n",
       " '<|iw|>',\n",
       " '<|uk|>',\n",
       " '<|el|>',\n",
       " '<|ms|>',\n",
       " '<|cs|>',\n",
       " '<|ro|>',\n",
       " '<|da|>',\n",
       " '<|hu|>',\n",
       " '<|ta|>',\n",
       " '<|no|>',\n",
       " '<|th|>',\n",
       " '<|ur|>',\n",
       " '<|hr|>',\n",
       " '<|bg|>',\n",
       " '<|lt|>',\n",
       " '<|la|>',\n",
       " '<|mi|>',\n",
       " '<|ml|>',\n",
       " '<|cy|>',\n",
       " '<|sk|>',\n",
       " '<|te|>',\n",
       " '<|fa|>',\n",
       " '<|lv|>',\n",
       " '<|bn|>',\n",
       " '<|sr|>',\n",
       " '<|az|>',\n",
       " '<|sl|>',\n",
       " '<|kn|>',\n",
       " '<|et|>',\n",
       " '<|mk|>',\n",
       " '<|br|>',\n",
       " '<|eu|>',\n",
       " '<|is|>',\n",
       " '<|hy|>',\n",
       " '<|ne|>',\n",
       " '<|mn|>',\n",
       " '<|bs|>',\n",
       " '<|kk|>',\n",
       " '<|sq|>',\n",
       " '<|sw|>',\n",
       " '<|gl|>',\n",
       " '<|mr|>',\n",
       " '<|pa|>',\n",
       " '<|si|>',\n",
       " '<|km|>',\n",
       " '<|sn|>',\n",
       " '<|yo|>',\n",
       " '<|so|>',\n",
       " '<|af|>',\n",
       " '<|oc|>',\n",
       " '<|ka|>',\n",
       " '<|be|>',\n",
       " '<|tg|>',\n",
       " '<|sd|>',\n",
       " '<|gu|>',\n",
       " '<|am|>',\n",
       " '<|yi|>',\n",
       " '<|lo|>',\n",
       " '<|uz|>',\n",
       " '<|fo|>',\n",
       " '<|ht|>',\n",
       " '<|ps|>',\n",
       " '<|tk|>',\n",
       " '<|nn|>',\n",
       " '<|mt|>',\n",
       " '<|sa|>',\n",
       " '<|lb|>',\n",
       " '<|my|>',\n",
       " '<|bo|>',\n",
       " '<|tl|>',\n",
       " '<|mg|>',\n",
       " '<|as|>',\n",
       " '<|tt|>',\n",
       " '<|haw|>',\n",
       " '<|ln|>',\n",
       " '<|ha|>',\n",
       " '<|ba|>',\n",
       " '<|jw|>',\n",
       " '<|su|>',\n",
       " '<|translate|>',\n",
       " '<|transcribe|>',\n",
       " '<|startoflm|>',\n",
       " '<|startofprev|>',\n",
       " '<|nocaptions|>',\n",
       " '<|notimestamps|>']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specials = processor.tokenizer.special_tokens_map[\"additional_special_tokens\"]\n",
    "specials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a68e6719-28b5-4175-8dba-8223ac68e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51865\n"
     ]
    }
   ],
   "source": [
    "special_tokens = {\"additional_special_tokens\": [\"<|speakerturn|>\"]}\n",
    "print(len(processor.tokenizer))\n",
    "processor.tokenizer.add_special_tokens(special_tokens, replace_additional_special_tokens=False)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "print(len(processor.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde95c6a-d76b-4ab6-843a-b18f252e8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca00a374-dcf0-49a0-9719-2f205089aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbc037ef-ec9b-4985-8f27-299e245921f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b9e72e6-6d79-4c07-a9c9-c2fa340893da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight torch.Size([384, 8])\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight torch.Size([8, 384])\n",
      "base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight torch.Size([384, 8])\n"
     ]
    }
   ],
   "source": [
    "for name, param in peft_model.named_parameters():\n",
    "    if \"lora\" in name.lower():\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1fb098-745f-4752-a9f8-fc96877de918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator test:\n",
      "Input features shape: torch.Size([1, 80, 3000])\n",
      "Labels shape: torch.Size([1, 448])\n"
     ]
    }
   ],
   "source": [
    "def data_collator(features):\n",
    "    \"\"\"\n",
    "    Processes a list of training samples and returns a batch dictionary.\n",
    "    Each sample should have:\n",
    "      - \"audio\": a dict with keys \"array\" and \"sampling_rate\"\n",
    "      - \"text\": the target transcript (with <|speakerturn|> tokens)\n",
    "    \"\"\"\n",
    "    input_features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for f in features:\n",
    "        # Process audio into log-mel spectrogram features.\n",
    "        audio_array = f[\"audio\"][\"array\"]\n",
    "        sample_rate = f[\"audio\"][\"sampling_rate\"]\n",
    "        inputs = processor(audio_array, sampling_rate=sample_rate, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "        # Squeeze extra dimensions so that input_features is 2D: (feature_length, hidden_size)\n",
    "        input_features_list.append(inputs.input_features.squeeze(0))\n",
    "        \n",
    "        # Tokenize the target text; pad/truncate to a fixed max_length (e.g., 512 tokens).\n",
    "        tokenized = processor.tokenizer(f[\"text\"], truncation=True, max_length=448, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels_list.append(tokenized.input_ids.squeeze(0))\n",
    "    \n",
    "    batch = {\n",
    "        \"input_features\": torch.stack(input_features_list),\n",
    "        \"labels\": torch.stack(labels_list)\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# Test the data collator with one sample from your dataset (ds is your dataset from the generator)\n",
    "# For example, if ds is already created as shown in the previous step:\n",
    "sample = ds[0]\n",
    "collated = data_collator([sample])\n",
    "print(\"Data collator test:\")\n",
    "print(\"Input features shape:\", collated[\"input_features\"].shape)\n",
    "print(\"Labels shape:\", collated[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9815e43c-fb4a-44e6-97e9-9d097a5658e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-diarization-lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41a3b99a-9769-4cd5-a1e1-49bdcc5641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.generation_config = peft_model.generation_config\n",
    "training_args.data_collator = data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69a8610b-fb84-4f3b-8ac2-755de4cf3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds,  # your dataset created from the generator\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b718b10a-9907-4af1-ab4b-c475669ab578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/77 10:27 < 28:45, 0.03 it/s, Epoch 0.27/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.611400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/peft/peft_model.py:849\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    848\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1766\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1762\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1763\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1764\u001b[0m         )\n\u001b[0;32m-> 1766\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1786\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1633\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1627\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1628\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1629\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1630\u001b[0m     )\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1633\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:1323\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1309\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1310\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1311\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         cache_position,\n\u001b[1;32m   1321\u001b[0m     )\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1323\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1336\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:715\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    712\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    724\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:525\u001b[0m, in \u001b[0;36mWhisperSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(current_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m--> 525\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;66;03m# save all key/value_states to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[1;32m    528\u001b[0m         cache_position \u001b[38;5;241m=\u001b[39m cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlx2/lib/python3.12/site-packages/transformers/models/whisper/modeling_whisper.py:268\u001b[0m, in \u001b[0;36mWhisperAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9eabd-911d-4ccb-af20-cc67c8176c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c707aa83-11ef-4e2e-a958-dd16a1cad670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meeting_id': 'ES2011a',\n",
       " 'position': 0,\n",
       " 'audio': {'path': None,\n",
       "  'array': array([-2.74658203e-04, -3.05175781e-04, -2.13623047e-04, ...,\n",
       "          9.15527344e-05,  9.15527344e-05,  9.15527344e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'text': \"HERE WE GO WELCOME EVERYBODY UM I'M ABIGAIL CLAFLIN YOU CAN CALL ME ABBIE 'S SEE POWERPOINT THAT'S NOT IT THERE WE GO SO THIS IS OUR KICK OFF MEETING UM AND I GUESS WE SHOULD ALL GET ACQUAINTED\",\n",
       " 'duration': 24.149993896484375}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
